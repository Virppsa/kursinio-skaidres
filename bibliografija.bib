Plačiau apie šaltinių tipus ir jų atributus:
http://mirror.datacenter.by/pub/mirrors/CTAN/macros/latex/contrib/biblatex/doc/biblatex.pdf#subsection.2.1

@article{AICollapseNature,
	title = {{AI} models collapse when trained on recursively generated data},
	volume = {631},
	rights = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07566-y},
	doi = {10.1038/s41586-024-07566-y},
	abstract = {\&nbsp;Analysis shows that indiscriminately training generative artificial intelligence on real and generated content, usually done by scraping data from\&nbsp;the Internet, can lead to a collapse in the ability of the models to generate diverse high-quality output.},
	pages = {755--759},
	number = {8022},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Papernot, Nicolas and Anderson, Ross and Gal, Yarin},
	date = {2024-07},
	langid = {english},
}


@misc{CurseOfRecursion,
	title = {The Curse of Recursion: Training on Generated Data Makes Models Forget},
	url = {http://arxiv.org/abs/2305.17493},
	doi = {10.48550/arXiv.2305.17493},
	shorttitle = {The Curse of Recursion},
	abstract = {Stable Diffusion revolutionised image creation from descriptive text. {GPT}-2, {GPT}-3(.5) and {GPT}-4 demonstrated astonishing performance across a variety of language tasks. {ChatGPT} introduced such language models to the general public. It is now clear that large language models ({LLMs}) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to {GPT}-\{n\} once {LLMs} contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and {LLMs}. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by {LLMs} in data crawled from the Internet.},
	number = {{arXiv}:2305.17493},
	publisher = {{arXiv}},
	author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
	date = {2024-04-14},
	eprinttype = {arxiv},
	eprint = {2305.17493 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}




@inproceedings{ModelsGoMAD,
	title = {Self-Consuming Generative Models Go {MAD}},
	url = {https://openreview.net/forum?id=ShjMHfmPs0},
	abstract = {Seismic advances in generative {AI} algorithms for imagery, text, and other data types have led to the temptation to use {AI}-synthesized data to train next-generation models. Repeating this process creates an autophagous ("self-consuming") loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and whether the samples from previous-generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that *without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease.* We term this condition Model Autophagy Disorder ({MAD}), by analogy to mad cow disease, and show that appreciable {MADness} arises in just a few generations.},
	eventtitle = {The Twelfth International Conference on Learning Representations},
	author = {Alemohammad, Sina and Casco-Rodriguez, Josue and Luzi, Lorenzo and Humayun, Ahmed Imtiaz and Babaei, Hossein and {LeJeune}, Daniel and Siahkoohi, Ali and Baraniuk, Richard},
	urldate = {2025-01-15},
	date = {2023-10-13},
	langid = {english},
}


@article{Melanoma,
	title = {Artificial Intelligence in Skin Cancer Detection: Recent Advances and Future Directions},
	issn = {2250-1754},
	url = {https://doi.org/10.1007/s40009-024-01545-7},
	doi = {10.1007/s40009-024-01545-7},
	shorttitle = {Artificial Intelligence in Skin Cancer Detection},
	abstract = {Skin cancer is a major global public health concern. Early diagnosis of skin cancer is essential for better patient outcomes and lower mortality rates. Artificial intelligence ({AI}) has the potential to increase the precision of skin cancer prediction and help doctors with diagnosis. This short communication reviews the recent advances and future directions in {AI}-powered skin cancer prediction. A comprehensive assessment of the literature is conducted to evaluate papers published between 2016 and 2022. The proposed research methodology explored Google Scholar, Scopus, and Web of Science databases to identify relevant studies according to proposed research questions. A total of 780 studies were found, of which 62 were from Scopus and 20 were from Web of Science These studies were selected for study as per the research question constraints. The study examined how well {AI} could distinguish between benign and malignant lesions and diagnose skin cancer, including melanoma and non- melanoma skin cancers. The analysis showed that deep learning algorithms showed good rates of sensitivity and specificity in predicting skin cancer, suggesting that they could be a useful tool for dermatologists. However, the small sample sizes and lack of real-world testing limited the generalizability of these findings. Additional research is needed to assess how well {AI} works on people with various skin tones under different environmental conditions. Future research should also focus on developing {AI} algorithms that can be used to diagnose other types of skin cancer, such as basal cell carcinoma and squamous cell carcinoma.},
	journaltitle = {National Academy Science Letters},
	shortjournal = {Natl. Acad. Sci. Lett.},
	author = {Kumari, Gayatri and Joshi, Vijay Kumar},
	date = {2024-11-29},
	langid = {english},
	keywords = {Artificial Intelligence, Artificial intelligence, Deep learning, Healthcare, Melanoma, Skin cancer},
}



@article{Drebejimai,
	title = {Application of Artificial Intelligence in Predicting Earthquakes: State-of-the-Art and Future Challenges},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9218936/?arnumber=9218936},
	doi = {10.1109/ACCESS.2020.3029859},
	shorttitle = {Application of Artificial Intelligence in Predicting Earthquakes},
	abstract = {Predicting the time, location and magnitude of an earthquake is a challenging job as an earthquake does not show specific patterns resulting in inaccurate predictions. Techniques based on Artificial Intelligence ({AI}) are well known for their capability to find hidden patterns in data. In the case of earthquake prediction, these models also produce a promising outcome. This work systematically explores the contributions made to date in earthquake prediction using {AI}-based techniques. A total of 84 scientific research papers, which reported the use of {AI}-based techniques in earthquake prediction, have been selected from different academic databases. These studies include a range of {AI} techniques including rule-based methods, shallow machine learning and deep learning algorithms. Covering all existing {AI}-based techniques in earthquake prediction, this article provides an account of the available methodologies and a comparative analysis of their performances. The performance comparison has been reported from the perspective of used datasets and evaluation metrics. Furthermore, using comparative analysis of performances the paper aims to facilitate the selection of appropriate techniques for earthquake prediction. Towards the end, it outlines some open challenges and potential research directions in the field.},
	pages = {192880--192923},
	journaltitle = {{IEEE} Access},
	author = {Banna, Md. Hasan Al and Taher, Kazi Abu and Kaiser, M. Shamim and Mahmud, Mufti and Rahman, Md. Sazzadur and Hosen, A. S. M. Sanwar and Cho, Gi Hwan},
	date = {2020},
	keywords = {{AI}, Earth, Earthquakes, Machine learning, Neural networks, Prediction algorithms, Predictive models, deep learning, earthquake, machine learning, review},
}


@article{Cunamiai,
	title = {Coastal tsunami prediction in Tohoku region, Japan, based on S-net observations using artificial neural network},
	volume = {75},
	issn = {1880-5981},
	url = {https://doi.org/10.1186/s40623-023-01912-6},
	doi = {10.1186/s40623-023-01912-6},
	abstract = {We present a novel method for coastal tsunami prediction utilizing a denoising autoencoder ({DAE}) model, one of the deep learning algorithms. Our study focuses on the Tohoku coast, Japan, where dense offshore bottom pressure gauges ({OBPGs}), called S-net, are installed. To train the model, we generated 800 hypothetical tsunami scenarios by employing stochastic earthquake models (M7.0–8.8). We used synthetic tsunami waveforms at 44 {OBPGs} as input and the waveforms at four coastal tide gauges as output. Subsequently, we evaluated the model’s performance using 200 additional hypothetical and two real tsunami events: the 2016 Fukushima earthquake and 2022 Tonga volcanic tsunamis. Our {DAE} model demonstrated high accuracy in predicting coastal tsunami waveforms for hypothetical events, achieving an impressive quality index of approximately 90\%. Furthermore, it accurately forecasted the maximum amplitude of the 2016 Fukushima tsunami, achieving a quality index of 91.4\% at 15 min after the earthquake. However, the prediction of coastal waveforms for the 2022 Tonga volcanic tsunami was not satisfactory. We also assessed the impact of the forecast time window and found that it had limited effects on forecast accuracy. This suggests that our method is suitable for providing rapid forecasts soon after an earthquake occurs. Our research is the first application of an artificial neural network to tsunami prediction using real observations. In the future, we will use more tsunami scenarios for model training to enhance its robustness for different types of tsunamis.},
	pages = {154},
	number = {1},
	journaltitle = {Earth, Planets and Space},
	shortjournal = {Earth, Planets and Space},
	author = {Wang, Yuchen and Imai, Kentaro and Miyashita, Takuya and Ariyoshi, Keisuke and Takahashi, Narumi and Satake, Kenji},
	urldate = {2025-01-10},
	date = {2023-10-08},
	keywords = {2016 Fukushima tsunami, 2022 Tonga volcanic tsunami, Denoising autoencoder model, Offshore bottom pressure gauge, Tsunami early warning},
}



@book{Statistika,
	location = {London, United Kingdom :},
	edition = {Sixth edition.},
	title = {Introduction to probability and statistics for engineers and scientists /},
	publisher = {Academic Press,},
	author = {Ross, Sheldon M.},
	date = {2021},
}



@misc{DesniuPasiulymai,
	title = {A Tale of Tails: Model Collapse as a Change of Scaling Laws},
	url = {http://arxiv.org/abs/2402.07043},
	doi = {10.48550/arXiv.2402.07043},
	shorttitle = {A Tale of Tails},
	abstract = {As {AI} model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation using the large language model Llama2.},
	number = {{arXiv}:2402.07043},
	publisher = {{arXiv}},
	author = {Dohmatob, Elvis and Feng, Yunzhen and Yang, Pu and Charton, Francois and Kempe, Julia},
	date = {2024-05-31},
	eprinttype = {arxiv},
	eprint = {2402.07043 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}



@misc{energy_2019,
	title = {Energy and Policy Considerations for Deep Learning in {NLP}},
	url = {http://arxiv.org/abs/1906.02243},
	doi = {10.48550/arXiv.1906.02243},
	abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many {NLP} tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of {NLP} researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for {NLP}. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in {NLP} research and practice.},
	number = {{arXiv}:1906.02243},
	publisher = {{arXiv}},
	author = {Strubell, Emma and Ganesh, Ananya and {McCallum}, Andrew},
	date = {2019-06-05},
	eprinttype = {arxiv},
	eprint = {1906.02243 [cs]},
	keywords = {Computer Science - Computation and Language},
}



@ARTICLE{MNIST,
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine}, 
  title={The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]}, 
  year={2012},
  volume={29},
  number={6},
  pages={141-142},
  keywords={Machine learning},
  doi={10.1109/MSP.2012.2211477}}


@misc{modelCollapseRef,
	title = {Model Collapse Demystified: The Case of Regression},
	url = {http://arxiv.org/abs/2402.07712},
	doi = {10.48550/arXiv.2402.07712},
	shorttitle = {Model Collapse Demystified},
	abstract = {In the era of proliferation of large language and image generation models, the phenomenon of "model collapse" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the setting of high-dimensional regression and obtain analytic formulae which quantitatively outline this phenomenon in a broad range of regimes. In the special case of polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.},
	number = {{arXiv}:2402.07712},
	publisher = {{arXiv}},
	author = {Dohmatob, Elvis and Feng, Yunzhen and Kempe, Julia},
	date = {2024-04-30},
	eprinttype = {arxiv},
	eprint = {2402.07712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}







@article{grokking,
	title = {Towards Understanding Grokking: An Effective Theory of Representation Learning},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html},
	shorttitle = {Towards Understanding Grokking},
	pages = {34651--34663},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Liu, Ziming and Kitouni, Ouail and Nolte, Niklas S. and Michaud, Eric and Tegmark, Max and Williams, Mike},
	urldate = {2025-01-15},
	date = {2022-12-06},
	langid = {english},
}


@book{DeepLearningPython,
	location = {Shelter Island, New York},
	edition = {First Edition},
	title = {Deep Learning with Python},
	isbn = {9781617294433},
	abstract = {{SummaryDeep} Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google {AI} researcher François Chollet, this book builds your understanding through intuitive explanations and practical examples.Purchase of the print book includes a free {eBook} in {PDF}, Kindle, and {ePub} formats from Manning Publications.About the {TechnologyMachine} learning has made remarkable progress in recent years. We went from near-unusable speech and image recognition, to near-human accuracy. We went from machines that couldn't beat a serious Go player, to defeating a world champion. Behind this progress is deep learning—a combination of engineering advances, best practices, and theory that enables a wealth of previously impossible smart applications.About the {BookDeep} Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google {AI} researcher François Chollet, this book builds your understanding through intuitive explanations and practical examples. You'll explore challenging concepts and practice with applications in computer vision, natural-language processing, and generative models. By the time you finish, you'll have the knowledge and hands-on skills to apply deep learning in your own projects. What's {InsideDeep} learning from first {principlesSetting} up your own deep-learning environment Image-classification {modelsDeep} learning for text and {sequencesNeural} style transfer, text generation, and image {generationAbout} the {ReaderReaders} need intermediate Python skills. No previous experience with Keras, Tensor Flow, or machine learning is required.About the {AuthorFrançois} Chollet works on deep learning at Google in Mountain View, {CA}. He is the creator of the Keras deep-learning library, as well as a contributor to the Tensor Flow machine-learning framework. He also does deep-learning research, with a focus on computer vision and the application of machine learning to formal reasoning. His papers have been published at major conferences in the field, including the Conference on Computer Vision and Pattern Recognition ({CVPR}), the Conference and Workshop on Neural Information Processing Systems ({NIPS}), the International Conference on Learning Representations ({ICLR}), and others.Table of {ContentsPART} 1 - {FUNDAMENTALS} {OF} {DEEP} {LEARNING} What is deep learning?Before we begin: the mathematical building blocks of neural networks Getting started with neural {networksFundamentals} of machine {learningPART} 2 - {DEEP} {LEARNING} {IN} {PRACTICEDeep} learning for computer {visionDeep} learning for text and {sequencesAdvanced} deep-learning best {practicesGenerative} deep {learningConclusionsappendix} A - Installing Keras and its dependencies on Ubuntuappendix B - Running Jupiter notebooks on an {EC}2 {GPU} instance.},
	pagetotal = {384},
	publisher = {Manning},
	author = {Chollet, Francois},
	date = {2017-12-22},
}
%------------





@techreport{AIEuropeanAct,
    author={{European Parliament} and {Council of the European Union}},
    title={Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024, laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139, (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)},
    institution={Official Journal of the European Union},
    year={2024},
    address={Brussels, Belgium},
    langid={english},
}



